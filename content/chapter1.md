## 一、计算机变革还没发生时

在旧金山以南、硅谷以北，远方的松树的地盘将会被橡树和无线电望远镜所取代。一种新兴文化正准备为人类思维打造一种新的媒介。当经过量产的原型产品到达我们的家、办公室和学校时，我们的生活将发生巨大的变化。

【更改】这些改变人类思维方式的机器中，有一类会逐渐演变成现在为人熟知的个人电脑，它们与今天的信息处理技术的关系，就像电视机之于15世纪的印刷机一样。就是虽然信息技术和电视机还没有到来，但是已经呼之欲出了。只需要再过不到十年，世界各地数以亿计的人将会共同努力，利用过去一个世纪里少数思想家们设想中的工具，创建新型的人类社会。

没有人知道在人类的发展进程中，这最终会是一件好事还是灾难。因为这种技术赋能之后的结果，将很大程度上取决于我们如何应对和处理它。人类的思维还不会被机器取代，至少在可预见的未来不会。但毫无疑问的是，这种“思维放大器”、智能工具和交互式电子社区将在全球范围内改变人们思考、学习和交流的方式。

看起来，这次由新技术引发的社会变革，可能会比500年前那次人类思维解放时（文艺复兴时期，西欧的思想解放运动，民众逐渐开始阅读书籍）具有更强烈的影响。活字印刷术发明后不到一个世纪，欧洲识字的群体从享有特权的少数人，逐步扩大到了绝大部分公民。人们的生活剧烈而迅速地发生着变化，机械印刷的出现，让人们有机会去接触更多信息 - 通过印刷的书籍，思想开始逃离精英的私人图书馆，进入了芸芸众生的世界。

书籍的真正价值来自于它们所创造的社群，这些睿智的社群在世界各地仍然活跃。印刷品成为用于传播有关化学和诗歌，进化与革命，民主与心理学，技术与工业以及许多其他概念的媒介，这大大超出了活字印刷技术的开创者的眼界。

研究中原型产品可能需要十年或者更长时间，才能进入到大规模生产为复杂电子设备的状态。因此自60年代以来计算机科学的惊人成就，所带来的早期影响才刚刚开始进入我们的生活。文字处理器，电脑游戏，教育软件和计算机图形学在十年前（注：大概是80年代中叶）才开始被大家所认识。但如今它们都已开始引领数十亿美元的产业了。尽管如此，专家们一致认为，最惊人的发展尚未到来。

那些仍然工作在这个行业的一些个人计算机的先驱者们不会忘记 - 这个时代的开端以及他们最初怀揣的梦想。那时候个人电脑的概念在计算机专家看来，是一个晦涩的异端邪说。30年前，绝大多数设计，制造，编程和使用计算机的人，都认同这个关于计算机在社会中的用途的观念 “计算机是用于数学计算的神秘设备” 。 对于非专业人士而言，计算机技术被认为过于精细，贵重并且很复杂。

在50年代，不同意这一观点的人屈指可数。对于计算机的未来，这些人有相似的愿景：所有人都有权享受由计算机带来的智能性和创造性，而不仅仅由技术专家独享。

有些人不认可计算机仅仅是用来处理数据的，虽然他们也同意计算机可以用来计算数据，但是这个过程应该具有更好的交互性。他们认为这工具可能有助于我们预测，构建和研究模型，帮助人们在不同方案之间进行选择以及帮助过滤出信息流中有用的部分。 他们想知道这种新生的设备除了充当做计算的工具，是否可能成为人类的通信媒介。

这些异端的计算机理论家提出，如果知识确实是力量，那么可以帮助我们将信息转化为知识的设备应该会成为是非常强大的技术的基础。 尽管大多数科学家和工程师对巨大的计算机器保持敬畏之心，但少数人坚持思考如何使用计算机以非数学方式辅助人类思维的操作。

本书将主要关注一些人物的想法，这些人在引领过往，现在以及未来的人机技术中产生了极大的影响。计算机史上的几个关键人物活跃在几个世纪或数十年前，我把这些人称为开创者（patriarchs），这些人在科学界享有盛誉，但鲜为常人所知。其他一些个人计算机技术的创造者仍活跃在这个领域，继续探索着人机交互的前沿，是当之无愧的开拓者（pioneers）。

那些即将体验这个新领域的年轻人，将会成为第一代 “互联网人”（infonauts）。现在讨论历史会如何看待这些新想法，还为时过早，但我们透过观察那些领域内正在产出新思路的人，希望找到一些线索，来推测不久的明天人们会怎么看待这个“新事物”。

我们将会慢慢发现，这项技术未来的限制将不在硬件上，而在我们的思想上。现代计算机是基于“通用机器”的理论演进而来的，“通用机器”实际上不是一种物理设备，而是对于一种能够模拟任何其他机器特征的机器的数学描述。一旦你创造了一台可以模仿任何其他机器的通用机器，这个工具的发展将超乎你的想象。因此机器是不是越来越智能不是我们短期的关注点，考虑如何更充分地利用这样的“通用机器”显得更为重要。

当今的个人计算机与未来的智能设备之间，硬件上的差别将远远不如用来控制计算机操作的软件上的变革，因为程序会告诉“通用机器”如何去模仿特定类型的机器。正如计算机的硬件基础从继电器到真空管，从晶体管再到集成电路一样，这些程序也在不断变革。当信息处理升级到知识处理时，真正的个人计算机将超脱于硬件，与人类的大脑联手，迸发出远超电子微型电路技术能设想出来的能量。

让我们忘掉“计算机知识”或晦涩难懂的技术术语吧，因为当机器和它们的程序变得更加智能时，这些“门槛”将会消失。首先，打造个人电脑的目的是为了让人们去做人类最擅长的事，即指挥机器去做机器擅长的事。许多人对计算机技术感到担忧，是因为他们被告知这些机器会比他们还要聪明，很多新手在刚接触计算机的时候都会被灌输这样的引导。事实上，这种焦虑应该是电脑需要承受的，一台笨到无法理解你想要什么的电脑，差不多就离被送进垃圾堆不远了。

Forget about "computer literacy" or obfuscating technical jargon, for these aberrations will disappear when the machines and their programs grow more intelligent. The reason for building a personal computer in the first place was to enable people to do what people do best be using machines to do what machines do best. Many people are afraid of today's computers because they have been told that these machines are smarter than they are--a deception that is reinforced by the rituals that novices have been forced to undergo in order to use computers. In fact, the burden of communication should be on the machine. A computer that is difficult to use is a computer that's too dumb to understand what you want.

If the predictions of some of the people in this book continue to be accurate, our whole environment will suddenly take on a kind of intelligence of its own sometime between now and the turn of the century. Fifteen years from now there will be a microchip in your telephone receiver with more computing power than all the technology the Defense Department can buy today. All the written knowledge in the world will be one of the items to be found in every schoolchild's pocket.

The computer of the twenty-first century will be everywhere, for better or for worse, and a more appropriate prophet than Orwell for this eventuality might well be Marshall McLuhan. Of McLuhan was tight about the medium being the message, what will it mean when the entire environment becomes the medium? If such development does occur as predicted, it will probably turn out differently from even the wildest "computerized household" scenarios of the recent past.

The possibility of accurately predicting the social impact of any new technology is questionable, to say the least.. At the beginning of the twentieth century, it was impossible for average people or even the most knowledgeable scientists to envision what life would be like for their grandchildren, who we now know would sit down in front of little boxes and watch events happening at that moment on the other side of the world.

Today, only a few people are thinking seriously about what to do with a living room wall that can tell you anything you want to know, simulate anything you want to see, connect you with any person or group of people you want to communicate with, and even help you find out what it is when you aren't entirely sure. In the 1990's it might be possible for people to "think as no human being has ever thought" and for computers to "process data in a way not approached by the information-handling machines we know today," as J.C.R. Licklider, one of the most influential pioneers, predicted in 1960, a quarter of a century before the hardware would begin to catch up with his ideas.

The earliest predictions about the impact of computing machinery occurred quite a bit earlier than 1960. The first electronic computers were invented by a few individuals, who often worked alone, during World War II. Before the actual inventors of the 1940s were the software patriarchs of the 1840s. And before them, thousands of years ago, the efforts of thinkers from many different cultures to find better ways to use symbols as tools led to the invention of mathematics and logic. It was these formal systems for manipulating symbols that eventually led to computation. Links in what we can now see as a continuous chain of thought were created by a series of Greek philosophers, British logocians, Hungarian mathematicians, and American inventors.

Most of the patriarchs had little in common with each other, socially or intellectually, but in some ways they were very much alike. It isn't surprising that they were exceptionally intelligent, but what is unusual is that they all seem to have been preoccupied with the power of their own minds. For sheer intellectual adventure, many intelligent people pursue the secrets of the stars, the mysteries of life, the myriad ways to use knowledge to accomplish practical goals. But what the software ancestors sought to create were tools to amplify the power of their own brains--machines to take over what they saw as the more mechanical aspects of thought.

Perhaps as an occupational hazard of this dangerously self-reflective enterprise, or as a result of being extraordinary people in restrictive social environments, the personalities of these patriarchs (and matriarchs) of computation reveal a common streak of eccentricity, ranging from the mildly unorthodox to the downright strange.

- Charles Babbage and Ada, Countess of Lovelace,.lived in the London of Dickens and Prince Albert (and knew them both). A hundred years before some of the best minds in the world used the resources of a nation to build a digital computer, these two eccentric inventor-mathematicians dreamed of building their "Analytical Engine." He constructed a partial prototype and she used it, with notorious lack of success, in a scheme to win a fortune at the horse races. Despite their apparent failures, Babbage was the first true computer designer, and Ada was history's first programmer.

- Gorge Boole invented a mathematical tool for future computer-builders--an "algebra of logic" that was used nearly a hundred years later to link the process of human reason to the operations of machines. The idea came to him in a flash of inspiration when he was walking across a meadow one day, at the age of seventeen, but it took him twenty years to teach himself enough mathematics to write The Laws of Thought.Although Boole's lifework was to translate his inspiration into an algebraic system, he continued to be so impressed with the suddenness and force of the revelation that hit him that day in the meadow that he also wrote extensively about the powers of the unconscious mind. After his death Boole's widow turned these ideas into a kind of human potential cult, a hundred years before the "me decade."

- Alan Turing.solved one of the most crucial mathematical problems of the modern era at the age of twenty-four, creating the theoretical basis for computation in the process. Then he became the top code-breaker in the world--when he wasn't bicycling around wearing a gas mask or running twenty miles with an alarm clock tied around his waist. If it hadn't been for the success of Turing's top-secret wartime mission, the Allies might have lost Worlds War II. After the war, he created the field of artificial intelligence and laid down the foundations of the art and science of programming.
He was notoriously disheveled, socially withdrawn, sometimes loud and abrasive, and even his friends thought that he carried nonconformity to weird extremes. At the age of forty-two, he committed suicide, hounded cruelly by the same government he helped save.

- John von Neumann.spoke five languages and knew dirty limericks in all of them. His colleagues, famous thinkers in their own right, all agreed that the operations of Johnny's mind were too deep and far too fast to be entirely human. He was one of history's most brilliant physicists, logicians, and mathematicians, as well as the software genius who invented the first electronic digital computer.John von Neumann was the center of the group who created the "stored program" concept that made truly powerful computers possible, and he specified a template that is still used to design almost all computers--the "von Neumann architecture." When he died, the Secretaries of Defense, the Army, Air Force, and Navy and the Joint Chiefs of staff were all gathered around his bed, attentive to his last gasps of technical and policy advice.

- Norbert Wiener,.raised to be a prodigy, graduated from tufts at fourteen, earned his Ph.D. from Harvard at eighteen, and studied with Bertrand Russell at nineteen. Wiener had a different kind of personality than his contemporary and colleague, von Neumann. Although involved in the early years of computers, he eventually refused to take part in research that could lead to the construction of weapons. Scarcely less brilliant than von Neumann, Wiener was vain, sometimes paranoid, and not known to be the life of the party, but he made important connections between computers, living organisms, and the fundamental laws of the physical universe. He guarded his ideas and feuded with other scientists, writing unpublished novels about mathematicians who did him wrong.Wiener's conception of cybernetics was partially derived from "pure" scientific work in mathematics, biology, and neurophysiology, and partially derived from the grimly applied science of designing automatic antiaircraft guns. Cybernetics was about the nature of control and communication systems in animals, humans, and machines.

- Claude Shannon,.another lone-wolf genius, is still known to his neighbors in Cambridge, Massachusetts, for his skill at riding a motorcycle. In 1937, as a twenty-one-year-old graduate student, he showed that Boole's logical algebra was the perfect tool for analyzing the complex networks of switching circuits used in telephone systems and, later, in computers. During the war and afterward, Shannon established the mathematical foundation of information theory. Together with cybernetics, this collection of theorems about information and communication created a new way to understand people and machines--and established information as a cosmic fundamental, along with energy and matter.

The software patriarchs came from wildly different backgrounds. Then as now, computer geniuses were often regarded as "odd" by those around them, and their reasons for wanting to invent computing devices seem to have been as varied as their personalities. Something about the notion of a universal machine enticed mathematicians and philosophers, logicians and code-breakers, whiz kids and bomb-builders. Even today, the worlds of computer research and the software business bring together an unlikely mixture of entrepreneurs and evangelists, futurians and utopians, cultists, obsessives, geniuses, pranksters, and fast-buck artists.

Despite their outward diversity, the computer patriarchs of a hundred years ago and the cyberneticians if the World War II era appear to have shared at least one characteristic with each other and with software pioneers and infonauts of more recent vintage. In recent years, the public has become more aware of a subculture that sprouted in Cambridge and Palo Alto and quietly spread through a national network of fluorescent-lit campus computer centers for the past two decades--the mostly young, mostly male, often brilliant sometimes bizarre "hackers," or self-confessed compulsive programmers. Sociologists and psychologists of the 1980's are only beginning to speculate about the deeper motivation for this obsession, but any later-day hacker will admit that the most fascinating thing in his own life is his own mind, and tell you that he regards intense, prolonged interaction with a computer program as a particularly satisfying kind of dialogue with his own thoughts.

A little touch of the hacker mentality seems to have affected all of the major players in this story. From what we know today about the patriarchs and pioneers, they all appear to have pursued a vision of a new way to use their minds. Each of them was trying to create a mental lever. Each of them contributed indispensable components of the device that was eventually assembled. But none of them encompassed it all.

The history of computation became increasingly complex as it progressed from the patriarchs to the pioneers. At the beginning, many of the earliest computer scientists didn't know that their ideas would end up in a kind of machine. Almost all of them worked in isolation. Because of their isolation from one another, the common intellectual ancestors of the modern computer are relatively easy to discern in retrospect. But since the 1950s, with the proliferation of researchers and teams of researchers in academic, industrial, and military institutions, the branches of the history have become tangled and too numerous to describe exhaustively. Since the 1950s, it has become increasingly difficult to assign credit for computer breakthroughs to individual inventors.

Although individual contributors to the past two or three decades of computer research development have been abundant, the people who have been able to see some kind of overall direction to the fast, fragmented progress of recent years have been sparse. Just as the earliest logicians and mathematicians didn't know their thoughts would end up as a part of a machine, the vast majority of the engineers and programmers of the 1960s were unaware that their machines had anything to do with human thought. The latter day computer pioneers in the middle chapters of this book were among the few who played central roles in the development of personal computing. Like their predecessors, these people tried to create a kind of mental lever. Unlike most of their predecessors, they were also trying to design a tool that the entire population might use.

Where the original software patriarchs solved various problems in the creation of the first computers, the personal computer pioneers struggled with equally vexing problems involved in using computers to create leverage for human intellect, the way wheels and dynamos create leverage for human muscles. Where the patriarchs were out to create computation, the pioneers sought to transform it:

- J.C.R. Licklider,.an experimental psychologist at MIT who became the director of the Information Processing Techniques Office of the U.S. Defense Department's Advanced Research Projects Agency (ARPA), was the one man whose vision enabled hundreds of other like-minded computer designers to pursue a whole new direction in hardware and software development. In the early 1960s, the researchers funded by Licklider's programs reconstructed computer science on a new and higher level, through an approach known as time-sharing.Although their sponsorship was military, the people Licklider hired or supported were working toward a transformation that he and they believed to be social as well as technological. Licklider saw the new breed of interactive computers his project directors were creating as the first step toward an entirely new kind of human communication capability.

- Doug Engelbart.started thinking about building a thought-amplifying device back when Harry Truman was President, and he has spent the last thirty years stubbornly pursuing his original vision of building a system for augmenting human intellect. At one point in the late 1960s, Engelbart and his crew of infonauts demonstrated to the assembled cream of computer scientists and engineers how the devices most people then used for performing calculations or keeping track of statistics could be used to enhance the most creative human activities.His former students have gone on to form a disproportionate part of the upper echelons of today's personal computer designers. Partially because of the myopia of his contemporaries, and partially because of his almost obsessive insistence on maintaining the purity of his original vision, most of Engelbart's innovations have yet to be adapted by the computer orthodoxy.

- Robert Taylor,.at the age of thirty-three, became the director of the ARPA office created by Licklider, thus launching his career in a new and much-needed field--the shaping of large-scale, long term, human-computer research campaigns. He became a "people collector," looking for those computer researchers whose ideas might have been ignored by the orthodoxy, but whose projects promised to boos the state of computer systems by orders of magnitude.

- Alan Kay.was one of television's original quiz kids. He learned to read at the age of two and a half, barely managed to avoid being thrown out of school and the Air Force, and ended up as a graduate student at one of the most important centers of ARPA research. In the 1970s, Kay was one of the guiding software spirits of PARC's Alto project (the first true personal computer) and the chief architect of Smalltalk, a new kind of computer language. He started the 1980s as a director of Atari Corporation's long-term research effort, and in 1984 he left Atari to become a "research fellow" for Apple Corporation.Along with his hard-won credentials as one of the rare original thinkers who is able to implement his thoughts via the craft of software design, Kay also has a reputation as a lifelong insubordinate. Since the first time he was thrown out of a classroom for knowing more than the teacher, Kay's avowed goal has been to build a "fantasy amplifier" that anyone with an imagination could use to explore the world of knowledge on their own, a "dynamic medium for creative thought" that could be as useful and thought-provocative to children in kindergarten as it would be to scientists in a research laboratory.

Licklider, Engelbart, Taylor, and Kay are still at work, confident that many more of us will experience the same thrill that has kept them going all these years--what Licklider, still at MIT, calls the "religious conversion" to interactive computing. Engelbart works for Tymshare Corporation, marketing his "Augment" system to information workers. Taylor is setting up another computer systems research center, this time under the auspices of the Digital Equipment Corporation, and is collecting people once again, this time for a research effort that will bring computing into the twenty-first century. Kay, at Atari, continued to steer toward the fantasy amplifier, despite the fact that their mother company was often described in the news media as "seriously troubled." It is fair to assume that he will continue to work toward the same goal in his new association with Steve Jobs, chairman of Apple and a computer visionary of a more entrepreneurial bent.

The pioneers, although they are still at work, are not the final characters in the story of the computer quest. The next generations of innovators are already at work, and some of them are surprisingly young. Computer trailblazers in the past tended to make their marks early in life--a trend that seems to be continuing in the present. Kay, the former quiz kid, is now in his early forties. Taylor is in his early fifties, Engelbart in his late fifties, and Licklider in his sixties. Today, younger men and, increasingly, younger women, have begun to take over the field professionally, while even younger generations are now living in their own versions of the future for fun, profit, and thrills.

The ones I call the "infonauts" are the older brothers and sisters of the adolescent hackers you read about in the papers. Most of them are in their twenties and thirties. They work for themselves or for some research institution or software house, and represent the first members of the McLuhan generation to use the technology invented by the von Neumann generation as tools to extend their imagination. From the science of designing what they call the "user interface"--where mind meets machine--to the art of building educational microworlds, the infonauts have been using their new medium to create the mass-media version we will use fifteen years from now.

- Avron Barr.is a knowledge engineer who helps build the special computer programs known as expert systems that are apparently able to acquire knowledge from human experts and transfer it to other humans. These systems are now used experimentally to help physicians diagnose diseases, as well as commercially to help geologists locate mineral deposits and to aid chemists in identifying new compounds. Although philosophers debate whether such programs truly "understand" what they are doing, and psychologists point out the huge gap between the narrowly defined kind of expertise involved in geology or diagnosis and the much more general "world knowledge" that all humans have, there is no denying that expert systems are valuable commodities. Avron Barr believes that they will evolve into more than expensive encyclopedias for specialists. In his midthirties and just starting his career in an infant technology, he dreams of creating an expert assistant in the art of helping people agree with one another.

- Brenda Laurel,.also in her midthirties, is an artist whose medium exists at the boundary of Kay's and Barr's and Engelbart's specialties. Her goal is to design new methods of play, learning, and artistic expression into computer-based technologies. Like Barr, she believes that the applications of her research point toward more extensive social effects than just another success in the software market.Brenda wants to use an expert system that knows what playwrights, composers, librarians, animators, artists, and dramatic critics know, to create a world of sights and sounds in which people can learn about flying a spaceship or surviving in the desert or being a blue whale by experiencing space-desert-whale simulated microworlds in person.

- Ted Nelson.is a dropout, gadfly, and self-proclaimed genius who self-published Computer Lib, the best-selling underground manifesto of the microcomputer revolution. His dream of a new kind of publishing medium and continuously updated world-library threatens to become the world's longest software project. He's wild and woolly, imaginative and hyperactive, has problems holding jobs and getting along with colleagues, and was the secret inspiration to all those subteenage kids who lashed together homebrew computers or homemade programs a few years back and are now the ruling moguls of the microcomputer industry.Time will tell whether he is a prophet too far ahead of his time, or just a persistent crackpot, but there is no doubt that he has contributed a rare touch of humor to the often too-serious world of computing. How can you not love somebody who says "they should have called it an oogabooga box instead of a computer"?

Despite their differences in background and personality, the computer patriarchs, software pioneers, and the newest breed of infonauts seem to share a distant focus on a future that they are certain the rest of us will see as clearly as they do--as soon as they turn what they see in their mind's eye into something we can hold in our hands. What did they see? What will happen when their visions materialize in our homes? And what do contemporary visionaries see in store for us next?

